BigQuery ML & Vertex AI CPU Prediction System

This project implements a machine learning pipeline to predict server CPU anomalies ("High CPU" > 80%) using Google Cloud BigQuery ML and Vertex AI.

It supports both Real-Time Alerts (via Vertex AI Endpoints) and Batch Reporting (via BigQuery SQL).

游 Strategic Decisions & Best Practices

1. Why NOT Retrain Every Night?

A common misconception is that "fresher is always better." In Machine Learning, retraining every 24 hours is generally discouraged for this use case because:

Model Thrashing: If yesterday was a weird anomaly (e.g., a national holiday or a network outage), retraining immediately makes your model "over-react" to that specific day. The model forgets the general rules and learns the noise.

Stability: You want a predictable alert system. If the model changes logic every night, your operations team cannot trust the alerts (e.g., "Why did this alert today but not yesterday?").

Cost: Training costs money (BigQuery slots). Retraining on 1 day of new data rarely improves accuracy enough to justify the daily cost.

Recommendation: Retrain Monthly or when accuracy explicitly drops (Data Drift). This captures legitimate long-term trend changes without over-reacting to daily noise.

2. Why NOT send server_id to Vertex AI?

When sending requests to the endpoint, we deliberately exclude the server_id.

It's not a predictive feature: A specific ID (like server_99) doesn't cause high CPU. The metrics (Load, Memory, I/O) cause high CPU. If you train on IDs, the model can't generalize to new servers it hasn't seen before.

The "Order Guarantee": Vertex AI endpoints guarantee that the order of the output matches the order of the input.

Input: [Metric A, Metric B, Metric C]

Output: [Prediction A, Prediction B, Prediction C]

The Solution: We keep the server_id in our client application (Python script) and "zip" it back together with the results after the API call returns.



### generate fake data 
```
-- 1. Empty the table first
TRUNCATE TABLE `ml_test.server_metrics`;

-- 2. Insert the new correlated data

INSERT INTO `formacionaiops-476808`.ml_test.server_metrics(server_id, timestamp_col, avg_cpu_last_hour,
  avg_cpu_yesterday, cpu_usage_percent)
WITH
  GENERATOR AS (
    SELECT
      x
    FROM
      UNNEST(GENERATE_ARRAY(1, 1000)) AS x
  ),
  RAW_VALUES AS (
    SELECT
      CONCAT('server_', CAST(FLOOR(1 + RAND() * 5) AS STRING)) AS server_id,
      TIMESTAMP_SUB(`CURRENT_TIMESTAMP`(), INTERVAL CAST(FLOOR(RAND() * 60 * 24) AS INT64) HOUR) AS timestamp_col,
      ROUND(RAND() * 100, 2) AS generated_last_hour,
      ROUND(RAND() * 100, 2) AS generated_yesterday
    FROM
      GENERATOR
  )
SELECT
  server_id,
  timestamp_col,
  generated_last_hour AS avg_cpu_last_hour,
  generated_yesterday AS avg_cpu_yesterday,
  LEAST(100, GREATEST(0, (generated_last_hour + (RAND() * 10 - 5)))) AS cpu_usage_percent
FROM
  RAW_VALUES;
```

###Create the new ML model 
```
CREATE OR REPLACE MODEL `ml_test.cpu_predictor_model`
OPTIONS(
  model_type = 'LOGISTIC_REG',
  input_label_cols = ['is_high_cpu'],
  model_registry = 'vertex_ai',
  vertex_ai_model_id = 'cpu_alert_v1' 
) AS
SELECT
  avg_cpu_last_hour,
  avg_cpu_yesterday,
  -- Re-calculating the label based on the new data
  IF(cpu_usage_percent > 80, 1, 0) AS is_high_cpu
FROM
  `ml_test.server_metrics`


```
Opci칩n 1: Consulta R치pida (Para ver la predicci칩n cruda)
Esta te dir치 1 (Alerta) o 0 (Normal) para cada fila de tu imagen.
``` Predecir valores BQ
SELECT
  server_id,
  timestamp_col,
  avg_cpu_last_hour,
  predicted_is_high_cpu -- 1 = CPU Alta (>80%), 0 = Normal
FROM
  ML.PREDICT(MODEL `ml_test.cpu_predictor_model`,
    (
      SELECT
        server_id,
        timestamp_col,
        avg_cpu_last_hour,
        avg_cpu_yesterday
      FROM
        `ml_test.server_metrics`
    )
  )
ORDER BY timestamp_col DESC;

```

``` Predecir valores BQ

Opci칩n 2: Consulta "Profesional" (Con Probabilidades)
Esta es mejor porque te dice qu칠 tan segura est치 la IA. Por ejemplo, te dir치: "Creo que es Alerta con un 95% de seguridad".


SELECT
  server_id,
  timestamp_col,
  avg_cpu_last_hour,
  
  -- Traducimos el 1 y 0 a texto
  CASE 
    WHEN predicted_is_high_cpu = 1 THEN '游댮 ALERTA'
    ELSE '游릭 NORMAL'
  END AS estado,

  -- Mostramos la seguridad de la predicci칩n (Probabilidad)
  ROUND(probs.prob * 100, 2) as probabilidad_de_alerta_pct

FROM
  ML.PREDICT(MODEL `ml_test.cpu_predictor_model`,
    (
      SELECT
        server_id,
        timestamp_col,
        avg_cpu_last_hour,
        avg_cpu_yesterday
      FROM
        `ml_test.server_metrics`
    )
  ),
  -- Desempaquetamos el array de probabilidades
  UNNEST(predicted_is_high_cpu_probs) as probs

WHERE probs.label = 1 -- Filtramos para ver la probabilidad de que sea "Alta"
ORDER BY probabilidad_de_alerta_pct DESC;

```

``` JSON
{
  "instances": [
    {
      "avg_cpu_last_hour": 95.5,
      "avg_cpu_yesterday": 88.0
    },
    {
      "avg_cpu_last_hour": 10.2,
      "avg_cpu_yesterday": 12.5
    }
  ]
}

```


Para este modelo espec칤fico de SQL (LOGISTIC_REG), la respuesta es: Predice el "Siguiente Paso Inmediato".

En detalle:

1. El Horizonte de Tiempo (쮺u치nto futuro?)
Este modelo NO te dice "Ma침ana a las 5 PM tendr치s un pico".

Este modelo responde a la pregunta: "Basado en lo que acaba de pasar (칰ltima hora), 쯘stoy en peligro AHORA MISMO o en los pr칩ximos minutos?".

Tu Dato de Entrada: avg_cpu_last_hour (Promedio de la 칰ltima hora).

Tu Predicci칩n: 쮼l siguiente dato que venga ser치 mayor de 80%?

El "futuro" depende de la frecuencia con la que llames al modelo:

Si le preguntas cada 5 minutos: Te est치 prediciendo el riesgo para los pr칩ximos 5 minutos.

Si le preguntas cada 1 hora: Te est치 prediciendo el riesgo para la siguiente hora.

2. Diferencia Clave: El Meteor칩logo vs. El M칠dico
Para que lo veas claro, comparemos los dos ejercicios que hemos hecho:

Caracter칤stica	Modelo 1 (Forecasting / Vertex AI)	Modelo 2 (Log. Regression / SQL)
Tipo	游댩 Meteor칩logo	游뽘 M칠dico de Urgencias
Pregunta	"쯈u칠 tiempo har치 ma침ana a las 10:00?"	"쮼ste paciente est치 a punto de infartar?"
Futuro	Largo plazo (24h, 48h, 7 d칤as...)	Inmediato / Corto plazo
Salida	Una curva de tiempo (Gr치fico)	Una Alerta (S칈/NO)
Uso ideal	Planificar recursos ("Comprar m치s servidores para ma침ana")	Actuar ya ("Reiniciar el servidor AHORA")

Export to Sheets

3. 쯇or qu칠 tu SQL funciona as칤?
F칤jate en la f칩rmula que usaste para generar los datos de entrenamiento en la SQL:

SQL

(generated_last_hour + (RAND() * 10 - 5))
Esto significa: "El valor futuro ser치 igual al de la 칰ltima hora, m치s/menos un peque침o cambio".

Por lo tanto, tu modelo ha aprendido que el futuro inmediato se parece mucho al presente. Si la 칰ltima hora fue un 95%, el modelo sabe que es matem치ticamente muy probable que el siguiente minuto siga siendo alto (Alerta 1).

Resumen
Este modelo LOGISTIC_REG te dice qu칠 probabilidad hay de que la CPU sature (>80%) en el siguiente ciclo de lectura.

Es perfecto para un sistema de Alertas en Tiempo Real.

Te avisa: "Oye, esto huele a quemado, va a fallar en breve".